%!TEX TS-program = xelatex
%!TEX options = -aux-directory=Debug -shell-escape -file-line-error -interaction=nonstopmode -halt-on-error -synctex=1 "%DOC%"
\documentclass{article}
\input{LaTeX-Submodule/template.tex}

% Additional packages & macros
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\divergence}{div}
\DeclareMathOperator{\curl}{curl}

% Header and footer
\newcommand{\unitName}{Electrical Engineering Mathematics}
\newcommand{\unitTime}{Semester 1, 2024}
\newcommand{\unitCoordinator}{Prof Scott McCue}
\newcommand{\documentAuthors}{Tarang Janawalkar}

\fancyhead[L]{\unitName}
\fancyhead[R]{\leftmark}
\fancyfoot[C]{\thepage}

% Copyright
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
    imagewidth={5em},
    hyphenation={raggedright}
]{doclicense}

\date{}

\begin{document}
%
\begin{titlepage}
    \vspace*{\fill}
    \begin{center}
        \LARGE{\textbf{\unitName}} \\[0.1in]
        \normalsize{\unitTime} \\[0.2in]
        \normalsize\textit{\unitCoordinator} \\[0.2in]
        \documentAuthors
    \end{center}
    \vspace*{\fill}
    \doclicenseThis
    \thispagestyle{empty}
\end{titlepage}
\newpage
%
\tableofcontents
\newpage
%
\part{Infinite Series}
\section{Sequences and Series}
\subsection{Sequences}
A sequence is an \textbf{ordered list} of numbers
\begin{equation*}
    a_1, \: a_2, \: a_3, \: \ldots, \: a_n, \: \ldots
\end{equation*}
denoted \({\left\{ a_n \right\}}_{n=1}^{\infty}\), where
\(n\) is the index of the sequence.
A sequence can be \textbf{finite} or \textbf{infinite}.
\subsection{Limits of Sequences}
An infinite sequence \(\left\{ a_n \right\}\) has a limit \(L\) if
\(a_n\) approaches \(L\) as \(n\) approaches infinity:
\begin{equation*}
    \lim_{n \to \infty} a_n = L
\end{equation*}
If such a limit exists, the sequence \textbf{converges} to \(L\).
Otherwise, the sequence \textbf{diverges}. Sequences that oscillate
between two or more values do not have a limit.
\subsection{Series}
Given a sequence \(\left\{ a_n \right\}\), we can construct a sequence
of \textbf{partial sums},
\begin{equation*}
    s_n = a_1 + a_2 + \cdots + a_n
\end{equation*}
denoted \(\left\{ s_n \right\}\), such that when \(\left\{ s_n \right\}\)
converges to a finite limit \(L\), that is,
\begin{equation*}
    \lim_{n \to \infty} s_n = L
\end{equation*}
the \textbf{infinite series} \(\sum_{n=1}^{\infty} a_n\) converges to \(L\).
Otherwise, the series \(\sum_{n=1}^{\infty} a_n\) diverges.
\subsubsection{Common Series}
Below are a list of common series that converge to a finite limit:
\begin{itemize}
    \item \textbf{Geometric Series}: A sum of the geometric progression
          \begin{equation*}
              \sum_{n=0}^{\infty} a r^n
          \end{equation*}
          converges when \(\abs*{r} < 1\), and diverges otherwise. When
          \(\abs*{r} < 1\),
          \begin{equation*}
              \sum_{n=0}^{\infty} a r^n = \frac{a}{1 - r}
          \end{equation*}
    \item \textbf{Harmonic Series}: A sum of the reciprocals of natural numbers
          \begin{equation*}
              \sum_{n=1}^{\infty} \frac{1}{n}
          \end{equation*}
          always diverges.
    \item \textbf{\(p\)-Series}: A sum of the reciprocals of \(p\)-powers of
          natural numbers
          \begin{equation*}
              \sum_{n=1}^{\infty} \frac{1}{n^p}
          \end{equation*}
          converges when \(p > 1\), and diverges otherwise. This series is
          closely related to the \textbf{Riemann Zeta Function}, and has
          exact values for even integers \(p\).
\end{itemize}
\subsection{Convergence Tests}
There are several tests to determine the convergence of an infinite
series. Note that these tests do not determine the value of the limit.
\subsubsection{Ratio Test}
Given the infinite series \(\sum_{n=1}^{\infty} a_n\), with
\begin{equation*}
    \rho = \lim_{n \to \infty} \abs*{\frac{a_{n+1}}{a_n}}
\end{equation*}
\begin{enumerate}[label=(\arabic*)]
    \item If \(\rho < 1\), the series converges.
    \item If \(\rho > 1\), the series diverges.
    \item If \(\rho = 1\), the test is inconclusive.
\end{enumerate}
\subsubsection{Alternating Series Test}
Given the infinite series \(\sum_{n=1}^{\infty} {\left( -1
\right)}^{n-1} b_n\), the alternating series converges if the following
conditions are met:
\begin{enumerate}[label=(\arabic*)]
    \item \(b_n > 0\) for all \(n\).
    \item \(b_{n+1} \leqslant b_n\) for all \(n\).
    \item \(\lim_{n \to \infty} b_n = 0\).
\end{enumerate}
\section{Taylor Series}
\subsection{Taylor Polynomials}
A Taylor polynomial is a polynomial that approximates a function near a
point \(x = x_0\). The \(n\)-th order Taylor polynomial of an
\(n\)-times differentiable function \(f\left( x \right)\) near \(x =
x_0\) is given by:
\begin{equation*}
    P_n\left( x \right) = f\left( x_0 \right) + f'\left( x_0 \right) \left( x - x_0 \right) + \frac{f''\left( x_0 \right)}{2!} {\left( x - x_0 \right)}^2 + \cdots + \frac{f^{\left( n \right)}\left( x_0 \right)}{n!} {\left( x - x_0 \right)}^n
\end{equation*}
Using summation notation, this becomes,
\begin{equation*}
    f\left( x \right) \approx P_n\left( x \right) = \sum_{k=0}^{n} \frac{f^{\left( k \right)}\left( x_0 \right)}{k!} {\left( x - x_0 \right)}^k
\end{equation*}
If \(f\) is (\(n+1\))-times differentiable on an interval including
\(x_0\), then the error of this approximation can be bounded by
\begin{equation*}
    R_n\left( x \right) = f\left( x \right) - P_n\left( x \right) = \frac{f^{\left( n+1 \right)}\left( p \right)}{\left( n+1 \right)!} {\left( x - x_0 \right)}^{n+1}
\end{equation*}
for some \(p\) between \(x\) and \(x_0\).
\subsection{The Taylor Series}
The Taylor polynomials can be extended to Taylor series by taking the
limit \(n \to \infty\). The Taylor series of an infinitely
differentiable function \(f\left( x \right)\) near \(x = x_0\) is
defined:
\begin{equation*}
    f\left( x \right) = \sum_{n=0}^{\infty} \frac{f^{\left( n \right)}\left( x_0 \right)}{n!} {\left( x - x_0 \right)}^n
\end{equation*}
When \(x_0 = 0\), the Taylor series is called the \textbf{Maclaurin series}.
\subsection{Convergence of Taylor Series}
The Taylor series is a form of a \textbf{power series}:
\begin{equation*}
    \sum_{n=0}^{\infty} c_n {\left( x - x_0 \right)}^n.
\end{equation*}
A power series may converge in one of three ways:
\begin{enumerate}[label=(\arabic*)]
    \item At a single point \(x = x_0\), with a radius of convergence
          \(R = 0\).
    \item On a finite open interval \(\ointerval{x_0 - R}{x_0 + R}\),
          with a radius of convergence \(R > 0\). The series is not
          guaranteed to converge at the endpoints of this interval.
    \item Everywhere, with a radius of convergence \(R = \infty\).
\end{enumerate}
For elementary functions, the Taylor series converges to the function
everywhere within the radius of convergence.
\subsection{Common Taylor Series}
Below are a list of common Taylor series expansions:
\begin{table}[H]
    \centering
    \begin{tabular}{c c c}
        \toprule
        \textbf{Function}             & \textbf{Taylor Series}                                                                           & \textbf{Convergence}     \\
        \midrule
        \(e^x\)                       & \(\displaystyle\sum_{n=0}^{\infty} \frac{x^n}{n!}\)                                              & \(-\infty < x < \infty\) \\
        \(\sin{\left( x \right)}\)    & \(\displaystyle\sum_{n=0}^{\infty} \frac{{\left( -1 \right)}^n x^{2n+1}}{\left( 2n+1 \right)!}\) & \(-\infty < x < \infty\) \\
        \(\cos{\left( x \right)}\)    & \(\displaystyle\sum_{n=0}^{\infty} \frac{{\left( -1 \right)}^n x^{2n}}{\left( 2n \right)!}\)     & \(-\infty < x < \infty\) \\
        \(\ln{\left( 1 - x \right)}\) & \(\displaystyle-\sum_{n=1}^{\infty} \frac{x^n}{n}\)                                              & \(-1 \leqslant x < 1\)   \\
        \(\ln{\left( 1 + x \right)}\) & \(\displaystyle\sum_{n=1}^{\infty} \frac{{\left( -1 \right)}^{n+1} x^n}{n}\)                     & \(-1 < x \leqslant 1\)   \\
        \(\dfrac{1}{1 - x}\)          & \(\displaystyle\sum_{n=0}^{\infty} x^n\)                                                         & \(-1 < x < 1\)           \\
        \bottomrule
    \end{tabular}
    % \caption{} % \label{}
\end{table}
\section{Fourier Series}
\subsection{Periodic Functions}
A function \(f\left( t \right)\) is \textbf{periodic} with period \(T\)
if it satisfies the following condition:
\begin{equation*}
    f\left( t + T \right) = f\left( t \right)
\end{equation*}
for all \(t\). As with Taylor polynomials, we wish to build an
approximation of \(f\left( t \right)\) using some basis.
\subsection{The Fourier Series}
The Fourier series is a representation of a periodic function as the
linear combination of sine and cosine functions. For a periodic
function \(f\left( t \right)\) with period \(T\), the Fourier series of
\(f\left( t \right)\) is defined:
\begin{equation*}
    f_F\left( t \right) = a_0 + \sum_{n=1}^{\infty} \left( a_n \cos{\left( \frac{2\pi n}{T} t \right)} + b_n \sin{\left( \frac{2\pi n}{T} t \right)} \right).
\end{equation*}
The coefficients \(a_0\), \(a_n\), and \(b_n\) are given by:
\begin{align*}
    a_0 & = \frac{1}{T} \int_{t_0}^{t_0 + T} f\left( t \right) \odif{t}                                         \\
    a_n & = \frac{2}{T} \int_{t_0}^{t_0 + T} f\left( t \right) \cos{\left( \frac{2\pi n}{T} t \right)} \odif{t} \\
    b_n & = \frac{2}{T} \int_{t_0}^{t_0 + T} f\left( t \right) \sin{\left( \frac{2\pi n}{T} t \right)} \odif{t}
\end{align*}
where \(t_0\) is any value of \(t\), often chosen to be \(0\) or \(-T/2\).
\subsection{Convergence of Fourier Series}
If \(f\left( t \right)\) is piecewise smooth on the interval
\(\interval{t_0}{t_0 + T}\), then the Fourier series converges to
\(f\left( t \right)\) in the interval \(\interval{t_0}{t_0 + T}\):
\begin{equation*}
    f_F\left( t \right) = \lim_{\epsilon \to 0^{+}} \frac{f\left( t + \epsilon \right) + f\left( t - \epsilon \right)}{2},
\end{equation*}
where discontinuous points \(\bar{t} \in
\interval{t_0}{t_0 + T}\) converge to the \textbf{average} of their
left-hand and right-hand limits. When \(f\) is non-periodic, the
Fourier series converges to the \textbf{periodic extension} of \(f\).
The endpoints of the interval may converge non-uniformly, corresponding
to jump discontinuities in the periodic extension of \(f\).
\subsection{Orthogonality}
Both the Taylor series and Fourier series are comprised of an
\textbf{orthogonal basis}. In this function space, the inner product of
two functions \(f\left( t \right)\) and \(g\left( t \right)\) is
defined:
\begin{equation*}
    \abracket*{f,\: g} = \int_{t_0}^{t_0 + T} f\left( t \right) g\left( t \right) \odif{t}
\end{equation*}
on the interval \(\interval{t_0}{t_0 + T}\). The norm of a function can
be defined as \(\norm{f} = \sqrt{\abracket*{f,\: f}}\). Using this
definition, we say that two functions are \textbf{orthogonal} if their
inner product is zero, and \textbf{orthonormal} if their inner product
is one.

The Fourier series is defined using an infinite dimensional set of
orthogonal basis functions:
\begin{equation*}
    \left\{ 1, \: \cos{\left( \frac{2\pi n}{T} t \right)}, \: \sin{\left( \frac{2\pi n}{T} t \right)} \right\}
\end{equation*}
for all \(n \in \mathbb{N}\). The inner products of these basis
functions are given by:
\begin{align*}
    \abracket*{\cos{\left( \frac{2\pi n}{T} t \right)},\: 1}                                       & = \int_{t_0}^{t_0 + T} \cos{\left( \frac{2\pi n}{T} t \right)} \odif{t} = 0                                         \\
    \abracket*{\sin{\left( \frac{2\pi n}{T} t \right)},\: 1}                                       & = \int_{t_0}^{t_0 + T} \sin{\left( \frac{2\pi n}{T} t \right)} \odif{t} = 0                                         \\
    \abracket*{\cos{\left( \frac{2\pi m}{T} t \right)},\: \sin{\left( \frac{2\pi n}{T} t \right)}} & = \int_{t_0}^{t_0 + T} \cos{\left( \frac{2\pi m}{T} t \right)} \sin{\left( \frac{2\pi n}{T} t \right)} \odif{t} = 0 \\
    \abracket*{\cos{\left( \frac{2\pi m}{T} t \right)},\: \cos{\left( \frac{2\pi n}{T} t \right)}} & =
    \int_{t_0}^{t_0 + T} \cos{\left( \frac{2\pi m}{T} t \right)} \cos{\left( \frac{2\pi n}{T} t \right)} \odif{t} =
    \begin{cases}
        \frac{T}{2}, & m = n    \\
        0            & m \neq n
    \end{cases}
    \\
    \abracket*{\sin{\left( \frac{2\pi m}{T} t \right)},\: \sin{\left( \frac{2\pi n}{T} t \right)}} & =
    \int_{t_0}^{t_0 + T} \sin{\left( \frac{2\pi m}{T} t \right)} \sin{\left( \frac{2\pi n}{T} t \right)} \odif{t} =
    \begin{cases}
        \frac{T}{2}, & m = n    \\
        0            & m \neq n
    \end{cases}
\end{align*}
for all \(m\) and \(n\) not equal to zero.
These results allow us to determine the Fourier series coefficients by
considering the inner product between \(f\left( t \right)\) and various
basis functions. For the coefficient \(a_0\), consider the inner product
of \(f\left( t \right)\) with the constant function \(1\):
\begin{align*}
    f\left( t \right)  & = a_0 + \sum_{n=1}^{\infty} \left( a_n \cos{\left( \frac{2\pi n}{T} t \right)} + b_n \sin{\left( \frac{2\pi n}{T} t \right)} \right)                                                      \\
    \abracket*{f,\: 1} & = a_0 \abracket*{1,\: 1} + \sum_{n=1}^{\infty} \left( a_n \abracket*{\cos{\left( \frac{2\pi n}{T} t \right)},\: 1} + b_n \abracket*{\sin{\left( \frac{2\pi n}{T} t \right)},\: 1} \right) \\
    a_0                & = \frac{\abracket*{f,\: 1}}{\abracket*{1,\: 1}} = \frac{1}{T} \int_{t_0}^{t_0 + T} f\left( t \right) \odif{t}
\end{align*}
For the coefficients \(a_n\) and \(b_n\), consider the inner product of
\(f\left( t \right)\) with \(\cos{\left( \frac{2\pi m}{T} t \right)}\)
and \(\sin{\left( \frac{2\pi m}{T} t \right)}\), respectively. For \(a_n\):
\begin{align*}
    f\left( t \right)                                        & = a_0 + \sum_{n=1}^{\infty} \left( a_n \cos{\left( \frac{2\pi n}{T} t \right)} + b_n \sin{\left( \frac{2\pi n}{T} t \right)} \right)                                                                                                                                     \\
    \abracket*{f,\: \cos{\left( \frac{2\pi m}{T} t \right)}} & =
    \begin{aligned}[t]
         & a_0 \abracket*{1,\: \cos{\left( \frac{2\pi m}{T} t \right)}}                                                                                                                                                                                 \\
         & + \sum_{n=1}^{\infty} \left( a_n \abracket*{\cos{\left( \frac{2\pi n}{T} t \right)},\: \cos{\left( \frac{2\pi m}{T} t \right)}} + b_n \abracket*{\sin{\left( \frac{2\pi n}{T} t \right)},\: \cos{\left( \frac{2\pi m}{T} t \right)}} \right)
    \end{aligned}
    \\
    a_m                                                      & = \frac{\abracket*{f,\: \cos{\left( \frac{2\pi m}{T} t \right)}}}{\abracket*{\cos{\left( \frac{2\pi m}{T} t \right)},\: \cos{\left( \frac{2\pi m}{T} t \right)}}} = \frac{2}{T} \int_{t_0}^{t_0 + T} f\left( t \right) \cos{\left( \frac{2\pi m}{T} t \right)} \odif{t}.
\end{align*}
For \(b_n\):
\begin{align*}
    f\left( t \right)                                        & = a_0 + \sum_{n=1}^{\infty} \left( a_n \cos{\left( \frac{2\pi n}{T} t \right)} + b_n \sin{\left( \frac{2\pi n}{T} t \right)} \right)                                                                                                                                     \\
    \abracket*{f,\: \sin{\left( \frac{2\pi m}{T} t \right)}} & =
    \begin{aligned}[t]
         & a_0 \abracket*{1,\: \sin{\left( \frac{2\pi m}{T} t \right)}}                                                                                                                                                                                 \\
         & + \sum_{n=1}^{\infty} \left( a_n \abracket*{\cos{\left( \frac{2\pi n}{T} t \right)},\: \sin{\left( \frac{2\pi m}{T} t \right)}} + b_n \abracket*{\sin{\left( \frac{2\pi n}{T} t \right)},\: \sin{\left( \frac{2\pi m}{T} t \right)}} \right)
    \end{aligned}
    \\
    b_m                                                      & = \frac{\abracket*{f,\: \sin{\left( \frac{2\pi m}{T} t \right)}}}{\abracket*{\sin{\left( \frac{2\pi m}{T} t \right)},\: \sin{\left( \frac{2\pi m}{T} t \right)}}} = \frac{2}{T} \int_{t_0}^{t_0 + T} f\left( t \right) \sin{\left( \frac{2\pi m}{T} t \right)} \odif{t}.
\end{align*}
\subsection{Even and Odd Functions}
A function \(f\left( t \right)\) is \textbf{even} if
\begin{equation*}
    f\left( -t \right) = f\left( t \right)
\end{equation*}
for all \(t\), and \textbf{odd} if
\begin{equation*}
    f\left( -t \right) = -f\left( t \right).
\end{equation*}
These functions have a special symmetry property that can be exploited
when computing integrals:
\begin{equation*}
    \int_{-T/2}^{T/2} f\left( t \right) \odif{t} =
    \begin{cases}
        2 \displaystyle\int_{0}^{T/2} f\left( t \right) \odif{t}, & \text{if \(f\left( t \right)\) even} \\
        0,                                                        & \text{if \(f\left( t \right)\) odd}
    \end{cases}
\end{equation*}
In the context of the Fourier series expansion, it is important to note
that cosine functions are even, and sine functions are odd:
\begin{align*}
    \cos{\left( -t \right)} & = \cos{\left( t \right)}  \\
    \sin{\left( -t \right)} & = -\sin{\left( t \right)}
\end{align*}
\subsection{Fourier Cosine Series}
Suppose \(f\left( t \right)\) is an even function with period \(T\),
and let us compute the Fourier series of \(f\left( t \right)\) on the
interval \(\interval{-T/2}{T/2}\). Consider the coefficients \(b_n\):
\begin{equation*}
    b_n = \frac{2}{T} \int_{-T/2}^{T/2} f\left( t \right) \sin{\left( \frac{2\pi n}{T} t \right)} \odif{t}
\end{equation*}
as \(f\left( t \right)\) is even, the resulting integrand is odd, and
the integral is zero. This results in a series containing only even
functions, called the Fourier cosine series expansion of
\(f\left( t \right)\):
\begin{align*}
    f_c\left( t \right) = a_0 + \sum_{n=1}^{\infty} a_n \cos{\left( \frac{2\pi n}{T} t \right)}
\end{align*}
with
\begin{align*}
    a_0 & = \frac{2}{T} \int_{0}^{T/2} f\left( t \right) \odif{t}                                         \\
    a_n & = \frac{4}{T} \int_{0}^{T/2} f\left( t \right) \cos{\left( \frac{2\pi n}{T} t \right)} \odif{t}
\end{align*}
\subsection{Fourier Sine Series}
Suppose \(f\left( t \right)\) is an odd function with period \(T\), and
let us compute the Fourier series of \(f\left( t \right)\) on the
interval \(\interval{-T/2}{T/2}\). Consider the coefficients \(a_0\)
and \(a_n\):
\begin{align*}
    a_0 & = \frac{1}{T} \int_{-T/2}^{T/2} f\left( t \right) \odif{t}                                         \\
    a_n & = \frac{2}{T} \int_{-T/2}^{T/2} f\left( t \right) \cos{\left( \frac{2\pi n}{T} t \right)} \odif{t}
\end{align*}
as \(f\left( t \right)\) is odd, the resulting integrand is odd for
both \(a_0\) and \(a_n\), and the integrals are zero. This results in a
series containing only odd functions, called the Fourier sine series
expansion of \(f\left( t \right)\):
\begin{align*}
    f_s\left( t \right) = \sum_{n=1}^{\infty} b_n \sin{\left( \frac{2\pi n}{T} t \right)}
\end{align*}
with
\begin{align*}
    b_n & = \frac{4}{T} \int_{0}^{T/2} f\left( t \right) \sin{\left( \frac{2\pi n}{T} t \right)} \odif{t}
\end{align*}
\subsection{Half-Range Expansions}
Suppose a function \(f\left( t \right)\) is defined on the interval
\(\interval{0}{T}\), that is not necessarily even or odd. We can extend
this function onto the interval in one of three ways:
\begin{itemize}
    \item Fourier series: Extends the function periodically on the
          interval \(\interval{0}{T}\), with period \(T\).
    \item Fourier cosine series: Extends the even expansion of the
          function on the interval \(\interval{-T}{T}\), with period
          \(2T\).
    \item Fourier sine series: Extends the odd expansion of the
          function on the interval \(\interval{-T}{T}\), with period
          \(2T\).
\end{itemize}
Note the period in the even and odd series must be twice the period of
the original function. This is illustrated in the figures below:
\begin{figure}[H]
    \centering
    \includegraphics[width = 0.8\linewidth]{figures/half_range_expansion_Fourier.pdf}
    \caption{Fourier series expansion of \(f\left( t \right)\) on the interval \(\interval{0}{T}\), with the period \(T\).} % \label{}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width = 0.8\linewidth]{figures/half_range_expansion_Cosine.pdf}
    \caption{Fourier cosine series expansion of \(f\left( t \right)\) onto the interval \(\interval{-T}{T}\), with the period \(2T\).} % \label{}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width = 0.8\linewidth]{figures/half_range_expansion_Sine.pdf}
    \caption{Fourier sine series expansion of \(f\left( t \right)\) onto the interval \(\interval{-T}{T}\), with the period \(2T\).} % \label{}
\end{figure}
\part{Vector Calculus}
\section{Scalar Fields}
A scalar field is any function \(f : \mathbb{R}^n \to \mathbb{R}\),
that assigns a scalar value to every vector in \(\mathbb{R}^n\).
\subsection{Partial Derivatives}
The partial derivatives of a scalar field are defined as the derivative
of the function with respect to each variable:
\begin{equation*}
    \pdv{f}{x_i} \equiv f_{x_i} = \lim_{h \to 0} \frac{f\left( x_1, \: \ldots, \: x_i + h, \: \ldots, \: x_n \right) - f\left( x_1, \: \ldots, \: x_i, \: \ldots, \: x_n \right)}{h}
\end{equation*}
that is, the rate of change of the function in the \(x_i\) direction,
holding all other variables constant.
\subsection{Directional Derivatives}
To find the rate of change of a scalar field \(f\left( x_1, \: \ldots,
\: x_n \right)\) in the direction of a unit vector \(\mathbf{u} =
\left[ u_1, \: \ldots, \: u_n \right]\), we can scale the standard
basis vectors by the components of \(\mathbf{u}\):
\begin{equation*}
    D_{\mathbf{u}} f \equiv \pdv{f}{\symbf{u}} = \sum_{i=1}^{n} \pdv{f}{x_i} \frac{u_i}{\norm*{\symbf{u}}}.
\end{equation*}
This is known as the \textbf{directional derivative} of \(f\) in the direction of \(\mathbf{u}\).
\subsection{Gradient}
The gradient of a scalar field is an operator \(\grad : f \to
\mathbb{R}^n\) which maps a scalar field \(f\) to a vector field:
\begin{equation*}
    \grad{f} \equiv \symbf{\nabla} f = \left[ \pdv{}{x_1}, \: \ldots, \: \pdv{}{x_n} \right].
\end{equation*}
We can equivalently write the directional derivative as the dot product
of the gradient of \(f\) with the unit vector \(\mathbf{u}\):
\begin{equation*}
    D_{\mathbf{u}} f = \symbf{\nabla} f \cdot \hat{\mathbf{u}}.
\end{equation*}
\subsection{Gradient of a Scalar Field}
The gradient of a scalar field \(f\) is a vector field that points in
the direction of the greatest rate of change of \(f\), with magnitude
equal to the rate of change. That is:
\begin{itemize}
    \item \(\symbf{\nabla} f\) points in the direction of greatest increase of \(f\).
    \item \(-\symbf{\nabla} f\) points in the direction of greatest decrease of \(f\).
    \item \(\norm*{\symbf{\nabla} f}\) is the rate of increase of \(f\) in that direction.
\end{itemize}
\section{Vector Fields}
A vector field is any function \(\symbf{F} : \mathbb{R}^n \to
\mathbb{R}^n\), that assigns a vector to every vector in
\(\mathbb{R}^n\).
\subsection{Partial Derivatives}
The partial derivatives of a vector field are defined as the partial
derivatives of each component of the vector field:
\begin{equation*}
    \pdv{\symbf{F}}{x_i} = \symbf{F}_{x_i} = \left[ \pdv{F_1}{x_i}, \: \ldots, \: \pdv{F_n}{x_i} \right]
\end{equation*}
\subsection{Divergence}
The divergence of a vector field is an operator \(\divergence :
\symbf{F} \to \mathbb{R}\), which maps a vector field \(\symbf{F}\) to
a scalar:
\begin{equation*}
    \divergence \symbf{F} = \symbf{\nabla} \cdot \symbf{F} = \sum_{i=1}^{n} \pdv{F_i}{x_i}.
\end{equation*}
The divergence of a vector field measures the rate at which the vector
field flows out of a point \(P\).
\begin{itemize}
    \item When \(\divergence \symbf{F} > 0\), the vector field tends to
          flows away from \(P\) (source).
    \item When \(\divergence \symbf{F} < 0\), the vector field tends to
          flows towards \(P\) (sink).
    \item When \(\divergence \symbf{F} = 0\), the net flow of the
          vector field at \(P\) is zero (conservative).
\end{itemize}
\subsection{Curl}
The curl of a vector field is an operator \(\curl : \symbf{F} \to
\symbf{G}\), which maps a vector field \(\symbf{F} : \mathbb{R}^3 \to
\mathbb{R}^3\) to another vector field \(\symbf{G} : \mathbb{R}^3 \to
\mathbb{R}^3\):
\begin{equation*}
    \curl \symbf{F} = \symbf{\nabla} \times \symbf{F} =
    \begin{vmatrix}
        \symbf{i}              & \symbf{j}              & \symbf{k}              \\
        \displaystyle\pdv{}{x} & \displaystyle\pdv{}{y} & \displaystyle\pdv{}{z} \\
        F_1                    & F_2                    & F_3
    \end{vmatrix}
    .
\end{equation*}
The curl may also be defined for vector fields in \(\mathbb{R}^2\), where
\(F_3 = 0\). The curl of a vector field measures the rotation of the
vector field at a point \(P\).
\begin{itemize}
    \item When \(\curl \symbf{F} > 0\), the vector field tends to
          rotate counterclockwise around \(P\).
    \item When \(\curl \symbf{F} < 0\), the vector field tends to
          rotate clockwise around \(P\).
    \item When \(\curl \symbf{F} = 0\), the net rotation of the vector
          field around \(P\) is zero.
\end{itemize}
\section{Multiple Integrals}
Scalar functions can be integrated over regions in \(\mathbb{R}^n\)
through multiple integrals.
\subsection{Double Integrals}
When integrating over some region \(R\) in \(\mathbb{R}^2\), consider
the small subregion \(R_{ij}\) with area \(\Delta A_i = \Delta x_i
\Delta y_i\), so that the double integral of a function \(f\left( x, \:
y \right)\) over \(R\) is defined as the contribution of each
subregion:
\begin{equation*}
    \iint_{R} f\left( x, \: y \right) \odif{A} = \lim_{N \to \infty} \sum_{i=1}^{N} f\left( x_i, \: y_i \right) \Delta A_i.
\end{equation*}
To compute this integral, we must bound the region by two functions
\(g\) and \(h\) in either the \(x\)- or \(y\)-direction.
\begin{itemize}
    \item In the \(y\)-direction, the region is bounded by the curves:
          \begin{alignat*}{2}
              g\left( x \right) & \hspace{0.5em} \leqslant \hspace{0.5em} y &  & \hspace{0.5em} \leqslant \hspace{0.5em} h\left( x \right) \\
              a                 & \hspace{0.5em} \leqslant \hspace{0.5em} x &  & \hspace{0.5em} \leqslant \hspace{0.5em} b
          \end{alignat*}
          for some functions \(g\left( x \right)\) and \(h\left( x \right)\)
          so that
          \begin{equation*}
              \iint_{R} f\left( x, \: y \right) \odif{A} = \int_a^b \left[ \int_{g\left( x \right)}^{h\left( x \right)} f\left( x, \: y \right) \odif{y} \right] \odif{x}.
          \end{equation*}
          Here we are adding up vertical strips of width \(\odif{x}\),
          where each strips height is given by the distance between
          \(g\left( x \right)\) and \(h\left( x \right)\), weighted by
          the function \(f\left( x, \: y \right)\).
    \item In the \(x\)-direction, the region is bounded by the curves:
          \begin{alignat*}{2}
              c                 & \hspace{0.5em} \leqslant \hspace{0.5em} x &  & \hspace{0.5em} \leqslant \hspace{0.5em} d                 \\
              g\left( y \right) & \hspace{0.5em} \leqslant \hspace{0.5em} x &  & \hspace{0.5em} \leqslant \hspace{0.5em} h\left( y \right)
          \end{alignat*}
          for some functions \(g\left( y \right)\) and \(h\left( y \right)\)
          so that
          \begin{equation*}
              \iint_{R} f\left( x, \: y \right) \odif{A} = \int_c^d \left[ \int_{g\left( y \right)}^{h\left( y \right)} f\left( x, \: y \right) \odif{x} \right] \odif{y}.
          \end{equation*}
          Here we are adding up horizontal strips of width \(\odif{y}\),
          where each strips height is given by the distance between
          \(g\left( y \right)\) and \(h\left( y \right)\), weighted by
          the function \(f\left( x, \: y \right)\).
\end{itemize}
\subsection{Order of Integration}
By Fubini's theorem, any permutation of the order of integration of an
iterated integral is equivalent if the function being integrated is
integrable, that is if:
\begin{equation*}
    \int_R \abs*{f\left( \symbf{x} \right)} \odif{\symbf{x}} < \infty.
\end{equation*}
When applying Fubini's theorem, we must appropriately modify the bounds
of integration to account for the region \(R\). For example, if the region
is bounded by the curves:
\begin{equation*}
    R = \left\{ \left( x, \: y \right) : a \leqslant x \leqslant b, \: g\left( x \right) \leqslant y \leqslant h\left( x \right) \right\},
\end{equation*}
where \(g\) and \(h\) are invertible on the interval \(\interval{a}{b}\),
and the integral of a function \(f\left( x, \: y \right)\) over \(R\)
is given by:
\begin{equation*}
    \iint_R f\left( x, \: y \right) \odif{A} = \int_a^b \left[ \int_{g\left( x \right)}^{h\left( x \right)} f\left( x, \: y \right) \odif{y} \right] \odif{x},
\end{equation*}
we can equivalently integrate over the region \(R\) by reversing the
order of integration:
\begin{equation*}
    \iint_R f\left( x, \: y \right) \odif{A} = \int_{g\left( a \right)}^{h\left( b \right)} \left[ \int_{h^{-1}\left( y \right)}^{g^{-1}\left( y \right)} f\left( x, \: y \right) \odif{x} \right] \odif{y}.
\end{equation*}
Similarly, if the region is bounded by the curves:
\begin{equation*}
    R = \left\{ \left( x, \: y \right) : c \leqslant y \leqslant d, \: g\left( y \right) \leqslant x \leqslant h\left( y \right) \right\},
\end{equation*}
we can integrate over the region \(R\) by reversing the order of
integration:
\begin{equation*}
    \iint_R f\left( x, \: y \right) \odif{A} = \int_{g\left( c \right)}^{h\left( d \right)} \left[ \int_{h^{-1}\left( x \right)}^{g^{-1}\left( x \right)} f\left( x, \: y \right) \odif{y} \right] \odif{x}.
\end{equation*}
\subsection{Triple Integrals}
When integrating over some volume \(V\) in \(\mathbb{R}^3\), consider
the small subregion \(V_{ijk}\) with volume \(\Delta V_i = \Delta x_i
\Delta y_i \Delta z_i\), so that the triple integral of a function
\(f\left( x, \: y, \: z \right)\) over \(V\) is defined as the
contribution of each subregion:
\begin{equation*}
    \iiint_{V} f\left( x, \: y, \: z \right) \odif{V} = \lim_{N \to \infty} \sum_{i=1}^{N} f\left( x_i, \: y_i, \: z_i \right) \Delta V_i.
\end{equation*}
To compute this integral, we require three intervals for each variable
\(x\), \(y\), and \(z\), that enclose the volume \(V\). As we introduce
another dimension, the function bounding the innermost integral may
depend on both the outer variables. This integral may take the form:
\begin{equation*}
    \iiint_{V} f\left( x, \: y, \: z \right) \odif{V} = \int_a^b \left[ \int_c^d \left[ \int_g^h f\left( x, \: y, \: z \right) \odif{z} \right] \odif{y} \right] \odif{x}.
\end{equation*}
for the volume enclosed by:
\begin{equation*}
    V = \left\{ \left( x, \: y, \: z \right) : a \leqslant x \leqslant b, \: c\left( x \right) \leqslant y \leqslant d\left( x \right), \: g\left( x, \: y \right) \leqslant z \leqslant h\left( x, \: y \right) \right\}.
\end{equation*}
Note that the bounds of any integral must not include any variables that
appear inside that integral. When modifying the order of integration,
we must ensure the same region is enclosed by the new bounds.
\subsection{Transformation of Coordinates}
In single variable calculus, we used a change of variables to simplify
integration by considering a transformation \(u = S\left( x \right)\),
to rewrite an integral in terms of \(u\), with the differential:
\begin{equation*}
    \odif{x} = \frac{1}{\odv{S\left( x \right)}{x}} \odif{u} = \odv{S^{-1}\left( u \right)}{u} \odif{u}
\end{equation*}
where \(x = S^{-1}\left( u \right)\) is the inverse transformation. This
concept can be extended to integrals with multiple variables by
considering the inverse transformation
\(x = T\left( u \right) = S^{-1}\left( u \right)\), where we can use
the chain rule to find the differential:
\begin{equation*}
    \odif{x} = \odv{T\left( u \right)}{u} \odif{u}.
\end{equation*}
To transform the coordinates in a multivariable integral, we must consider
a matrix of all the partial derivatives of a transformation. For the
transformation \(\symbf{x} = \symbf{T}\left( \symbf{u} \right)\),
consider the Jacobian matrix of partial derivatives:
\begin{equation*}
    \symbf{J} =
    \begin{bmatrix}
        \displaystyle\pdv{x_1}{u_1} & \cdots & \displaystyle\pdv{x_1}{u_n} \\
        \vdots                      & \ddots & \vdots                      \\
        \displaystyle\pdv{x_n}{u_1} & \cdots & \displaystyle\pdv{x_n}{u_n}
    \end{bmatrix}
    .
\end{equation*}
The determinant of this matrix is known as the Jacobian of a transformation,
and it gives us the factor by which the volume of the region is scaled
under the transformation, giving us the new differential:
\begin{equation*}
    \odif{\symbf{x}} = \abs*{\det{\symbf{J}}} \odif{\symbf{u}}.
\end{equation*}
Therefore, given a bijective transformation \(T : \Omega \subset \R^n \to \Omega'\subset \R^n\),
where \(T\) has continuous partial derivatives, an integral in
\(\symbf{x}\) can be transformed to an integral in \(\symbf{u}\) by:
\begin{equation*}
    \int_\Omega f\left( \symbf{x} \right) \odif{\symbf{x}} = \int_{\Omega'} f\left( \symbf{T}\left( \symbf{u} \right) \right) \abs*{\det{\symbf{J}}} \odif{\symbf{u}}.
\end{equation*}
\subsubsection{Polar Coordinates (2D)}
To transform a Cartesian coordinate system to polar coordinates,
consider the transformation:
\begin{align*}
    x & = r \cos{\theta} & r      & = \sqrt{x^2 + y^2}                    \\
    y & = r \sin{\theta} & \theta & = \arctan{\left( \frac{y}{x} \right)}
\end{align*}
for \(r \geqslant 0\) and \(0 \leqslant \theta \leqslant 2\pi\). This transformation has
the Jacobian matrix:
\begin{equation*}
    \symbf{J} =
    \begin{bmatrix}
        \displaystyle\pdv{x}{r} & \displaystyle\pdv{x}{\theta} \\
        \displaystyle\pdv{y}{r} & \displaystyle\pdv{y}{\theta}
    \end{bmatrix}
    =
    \begin{bmatrix}
        \cos{\theta} & -r \sin{\theta} \\
        \sin{\theta} & r \cos{\theta}
    \end{bmatrix}
\end{equation*}
with the determinant:
\begin{equation*}
    \abs*{\det{\symbf{J}}} = r \cos^2{\theta} + r \sin^2{\theta} = r.
\end{equation*}
Therefore, the differential in polar coordinates is:
\begin{equation*}
    \odif{x} \odif{y} = r \odif{r} \odif{\theta},
\end{equation*}
giving the integral transformation:
\begin{equation*}
    \iint_{R} f\left( x, \: y \right) \odif{x} \odif{y} = \iint_{R'} f\left( r, \: \theta \right) r \odif{r} \odif{\theta}.
\end{equation*}
The gradient of this transformation can be computed by applying the
chain rule:
\begin{align*}
    \pdv{}{x} & = \pdv{r}{x} \pdv{}{r} + \pdv{\theta}{x} \pdv{}{\theta} \\
    \pdv{}{y} & = \pdv{r}{y} \pdv{}{r} + \pdv{\theta}{y} \pdv{}{\theta}
\end{align*}
The partial derivatives in these expressions are given by:
\begin{align*}
    \pdv{r}{x} & = \frac{x}{r} = \cos{\theta} & \pdv{\theta}{x} & = -\frac{y}{r^2} = -\frac{1}{r}\sin{\theta} \\
    \pdv{r}{y} & = \frac{y}{r} = \sin{\theta} & \pdv{\theta}{y} & = \frac{x}{r^2} = \frac{1}{r}\cos{\theta}
\end{align*}
so that the gradient in polar coordinates is defined:
\begin{align*}
    \symbf{\nabla} & = \symbf{e}_x \pdv{}{x} + \symbf{e}_y \pdv{}{y}                                                                                                                                             \\
                   & = \symbf{e}_x \left[ \cos{\theta} \pdv{}{r} - \frac{1}{r} \sin{\theta} \pdv{}{\theta} \right] + \symbf{e}_y \left[ \sin{\theta} \pdv{}{r} + \frac{1}{r} \cos{\theta} \pdv{}{\theta} \right] \\
                   & = \left[ \cos{\theta} \symbf{e}_x + \sin{\theta} \symbf{e}_y \right] \pdv{}{r} + \left[ \cos{\theta} \symbf{e}_y - \sin{\theta} \symbf{e}_x \right] \frac{1}{r} \pdv{}{\theta}              \\
                   & = \symbf{e}_r \pdv{}{r} + \symbf{e}_\theta \frac{1}{r} \pdv{}{\theta}
\end{align*}
giving the transformed basis vectors:
\begin{align*}
    \symbf{e}_r      & = \cos{\theta} \symbf{e}_x + \sin{\theta} \symbf{e}_y \\
    \symbf{e}_\theta & = \cos{\theta} \symbf{e}_y - \sin{\theta} \symbf{e}_x
\end{align*}
\subsubsection{Cylindrical Coordinates (3D)}
To transform a Cartesian coordinate system to cylindrical coordinates,
consider the transformation:
\begin{align*}
    x & = r \cos{\theta} & r      & = \sqrt{x^2 + y^2}                    \\
    y & = r \sin{\theta} & \theta & = \arctan{\left( \frac{y}{x} \right)} \\
    z & = z              & z      & = z
\end{align*}
for \(r \geqslant 0\), \(0 \leqslant \theta \leqslant 2\pi\), and \(-\infty < z < \infty\).
This transformation has the Jacobian matrix:
\begin{equation*}
    \symbf{J} =
    \begin{bmatrix}
        \displaystyle\pdv{x}{r} & \displaystyle\pdv{x}{\theta} & \displaystyle\pdv{x}{z} \\
        \displaystyle\pdv{y}{r} & \displaystyle\pdv{y}{\theta} & \displaystyle\pdv{y}{z} \\
        \displaystyle\pdv{z}{r} & \displaystyle\pdv{z}{\theta} & \displaystyle\pdv{z}{z}
    \end{bmatrix}
    =
    \begin{bmatrix}
        \cos{\theta} & -r \sin{\theta} & 0 \\
        \sin{\theta} & r \cos{\theta}  & 0 \\
        0            & 0               & 1
    \end{bmatrix}
\end{equation*}
with the determinant:
\begin{equation*}
    \abs*{\det{\symbf{J}}} = r \cos^2{\theta} + r \sin^2{\theta} = r.
\end{equation*}
Therefore, the differential in cylindrical coordinates is:
\begin{equation*}
    \odif{x} \odif{y} \odif{z} = r \odif{r} \odif{\theta} \odif{z},
\end{equation*}
giving the integral transformation:
\begin{equation*}
    \iiint_{V} f\left( x, \: y, \: z \right) \odif{x} \odif{y} \odif{z} = \iiint_{V'} f\left( r, \: \theta, \: z \right) r \odif{r} \odif{\theta} \odif{z}.
\end{equation*}
The gradient of this transformation can be computed by applying the
chain rule:
\begin{align*}
    \pdv{}{x} & = \pdv{r}{x} \pdv{}{r} + \pdv{\theta}{x} \pdv{}{\theta} + \pdv{z}{x} \pdv{}{z} \\
    \pdv{}{y} & = \pdv{r}{y} \pdv{}{r} + \pdv{\theta}{y} \pdv{}{\theta} + \pdv{z}{y} \pdv{}{z} \\
    \pdv{}{z} & = \pdv{r}{z} \pdv{}{r} + \pdv{\theta}{z} \pdv{}{\theta} + \pdv{z}{z} \pdv{}{z}
\end{align*}
The partial derivatives in these expressions are given by:
\begin{align*}
    \pdv{r}{x} & = \frac{x}{r} = \cos{\theta} & \pdv{\theta}{x} & = -\frac{y}{r^2} = -\frac{1}{r}\sin{\theta} & \pdv{z}{x} & = 0 \\
    \pdv{r}{y} & = \frac{y}{r} = \sin{\theta} & \pdv{\theta}{y} & = \frac{x}{r^2} = \frac{1}{r}\cos{\theta}   & \pdv{z}{y} & = 0 \\
    \pdv{r}{z} & = 0                          & \pdv{\theta}{z} & = 0                                         & \pdv{z}{z} & = 1
\end{align*}
so that the gradient in cylindrical coordinates is defined:
\begin{align*}
    \symbf{\nabla} & = \symbf{e}_x \pdv{}{x} + \symbf{e}_y \pdv{}{y} + \symbf{e}_z \pdv{}{z}                                                                                                                                             \\
                   & = \symbf{e}_x \left[ \cos{\theta} \pdv{}{r} - \frac{1}{r} \sin{\theta} \pdv{}{\theta} \right] + \symbf{e}_y \left[ \sin{\theta} \pdv{}{r} + \frac{1}{r} \cos{\theta} \pdv{}{\theta} \right] + \symbf{e}_z \pdv{}{z} \\
                   & = \left[ \cos{\theta} \symbf{e}_x + \sin{\theta} \symbf{e}_y \right] \pdv{}{r} + \left[ \cos{\theta} \symbf{e}_y - \sin{\theta} \symbf{e}_x \right] \frac{1}{r} \pdv{}{\theta} + \symbf{e}_z \pdv{}{z}              \\
                   & = \symbf{e}_r \pdv{}{r} + \symbf{e}_\theta \frac{1}{r} \pdv{}{\theta} + \symbf{e}_z \pdv{}{z}
\end{align*}
giving the transformed basis vectors:
\begin{align*}
    \symbf{e}_r      & = \cos{\theta} \symbf{e}_x + \sin{\theta} \symbf{e}_y \\
    \symbf{e}_\theta & = \cos{\theta} \symbf{e}_y - \sin{\theta} \symbf{e}_x \\
    \symbf{e}_z      & = \symbf{e}_z
\end{align*}
\subsubsection{Spherical Coordinates (3D)}
To transform a Cartesian coordinate system to spherical coordinates,
consider the transformation:
\begin{align*}
    x & = r \cos{\theta} \sin{\phi} & r      & = \sqrt{x^2 + y^2 + z^2}              \\
    y & = r \sin{\theta} \sin{\phi} & \theta & = \arctan{\left( \frac{y}{x} \right)} \\
    z & = r \cos{\phi}              & \phi   & = \arccos{\left( \frac{z}{r} \right)}
\end{align*}
for \(r \geqslant 0\), \(0 \leqslant \theta \leqslant 2\pi\), and \(0 \leqslant \phi \leqslant \pi\).
This transformation has the Jacobian matrix:
\begin{equation*}
    \symbf{J} =
    \begin{bmatrix}
        \displaystyle\pdv{x}{r} & \displaystyle\pdv{x}{\phi} & \displaystyle\pdv{x}{\theta} \\
        \displaystyle\pdv{y}{r} & \displaystyle\pdv{y}{\phi} & \displaystyle\pdv{y}{\theta} \\
        \displaystyle\pdv{z}{r} & \displaystyle\pdv{z}{\phi} & \displaystyle\pdv{z}{\theta}
    \end{bmatrix}
    =
    \begin{bmatrix}
        \cos{\theta} \sin{\phi} & r \cos{\theta} \cos{\phi} & -r \sin{\theta} \sin{\phi} \\
        \sin{\theta} \sin{\phi} & r \sin{\theta} \cos{\phi} & r \cos{\theta} \sin{\phi}  \\
        \cos{\phi}              & -r \sin{\phi}             & 0
    \end{bmatrix}
\end{equation*}
with the determinant:
\begin{equation*}
    \abs*{\det{\symbf{J}}} = r^2 \sin{\phi}.
\end{equation*}
Therefore, the differential in spherical coordinates is:
\begin{equation*}
    \odif{x} \odif{y} \odif{z} = r^2 \sin{\phi} \odif{r} \odif{\phi} \odif{\theta},
\end{equation*}
giving the integral transformation:
\begin{equation*}
    \iiint_{V} f\left( x, \: y, \: z \right) \odif{x} \odif{y} \odif{z} = \iiint_{V'} f\left( r, \: \theta, \: \phi \right) r^2 \sin{\phi} \odif{r} \odif{\theta} \odif{\phi}.
\end{equation*}
The gradient of this transformation can be computed by applying the
chain rule:
\begin{align*}
    \pdv{}{x} & = \pdv{r}{x} \pdv{}{r} + \pdv{\phi}{x} \pdv{}{\phi} + \pdv{\theta}{x} \pdv{}{\theta} \\
    \pdv{}{y} & = \pdv{r}{y} \pdv{}{r} + \pdv{\phi}{y} \pdv{}{\phi} + \pdv{\theta}{y} \pdv{}{\theta} \\
    \pdv{}{z} & = \pdv{r}{z} \pdv{}{r} + \pdv{\phi}{z} \pdv{}{\phi} + \pdv{\theta}{z} \pdv{}{\theta}
\end{align*}
The partial derivatives in these expressions are given by:
\begin{align*}
    \pdv{r}{x} & = \frac{x}{r} = \cos{\theta} \sin{\phi} & \pdv{\phi}{x} & = -\frac{z\left( - x r^{-3} \right)}{\sqrt{1 - \frac{z^2}{r^2}}} = \frac{\cos{\theta}\cos{\phi}}{r} & \pdv{\theta}{x} & = -\frac{y}{x^2 + y^2} = -\frac{\sin{\theta}}{r \sin{\phi}} \\
    \pdv{r}{y} & = \frac{y}{r} = \sin{\theta} \sin{\phi} & \pdv{\phi}{y} & = -\frac{z\left( - y r^{-3} \right)}{\sqrt{1 - \frac{z^2}{r^2}}} = \frac{\sin{\theta}\cos{\phi}}{r} & \pdv{\theta}{y} & = \frac{x}{x^2 + y^2} = \frac{\cos{\theta}}{r \sin{\phi}}   \\
    \pdv{r}{z} & = \frac{z}{r} = \cos{\phi}              & \pdv{\phi}{z} & = -\frac{\left( r - z^2r^{-1} \right)r^{-2}}{\sqrt{1 - \frac{z^2}{r^2}}} = -\frac{\sin{\phi}}{r}    & \pdv{\theta}{z} & = 0
\end{align*}
so that the gradient in spherical coordinates is defined:
\begin{align*}
    \symbf{\nabla} & = \symbf{e}_x \pdv{}{x} + \symbf{e}_y \pdv{}{y} + \symbf{e}_z \pdv{}{z}                                                    \\
                   & =
    \begin{aligned}[t]
          & \symbf{e}_x \left[ \cos{\theta} \sin{\phi} \pdv{}{r} + \frac{\cos{\theta}\cos{\phi}}{r} \pdv{}{\phi} - \frac{\sin{\theta}}{r \sin{\phi}} \pdv{}{\theta} \right] \\
        + & \symbf{e}_y \left[ \sin{\theta} \sin{\phi} \pdv{}{r} + \frac{\sin{\theta}\cos{\phi}}{r} \pdv{}{\phi} + \frac{\cos{\theta}}{r \sin{\phi}} \pdv{}{\theta} \right] \\
        + & \symbf{e}_z \left[ \cos{\phi} \pdv{}{r} - \frac{\sin{\phi}}{r} \pdv{}{\phi} \right]
    \end{aligned}
    \\
                   & =
    \begin{aligned}[t]
          & \left[ \cos{\theta} \sin{\phi} \symbf{e}_x + \sin{\theta} \sin{\phi} \symbf{e}_y + \cos{\phi} \symbf{e}_z \right] \pdv{}{r}              \\
        + & \left[ \cos{\theta}\cos{\phi} \symbf{e}_x + \sin{\theta}\cos{\phi} \symbf{e}_y - \sin{\phi} \symbf{e}_z \right] \frac{1}{r} \pdv{}{\phi} \\
        + & \left[ \cos{\theta} \symbf{e}_y - \sin{\theta} \symbf{e}_x \right] \frac{1}{r \sin{\phi}} \pdv{}{\theta}
    \end{aligned}
    \\
                   & = \symbf{e}_r \pdv{}{r} + \symbf{e}_\phi \frac{1}{r} \pdv{}{\phi} + \symbf{e}_\theta \frac{1}{r \sin{\phi}} \pdv{}{\theta}
\end{align*}
giving the transformed basis vectors:
\begin{align*}
    \symbf{e}_r      & = \cos{\theta} \sin{\phi} \symbf{e}_x + \sin{\theta} \sin{\phi} \symbf{e}_y + \cos{\phi} \symbf{e}_z \\
    \symbf{e}_\phi   & = \cos{\theta}\cos{\phi} \symbf{e}_x + \sin{\theta}\cos{\phi} \symbf{e}_y - \sin{\phi} \symbf{e}_z   \\
    \symbf{e}_\theta & = \cos{\theta} \symbf{e}_y - \sin{\theta} \symbf{e}_x
\end{align*}
\subsection{Physical Interpretation of Integrals}

\section{Line and Surface Integrals}
For a curve \(\mathscr{C}\) defined by the parametric function
\(\symbf{r}\left( t \right) = \begin{bmatrix}
    x\left( t \right) \\
    y\left( t \right) \\
    z\left( t \right)
\end{bmatrix}\)
on the interval \(a \leqslant t \leqslant b\):
\begin{itemize}
    \item \textbf{Arc length}
          \begin{equation*}
              s\left( t \right) = \int_a^t \norm*{\symbf{r}'\left( \tau \right)} \odif{\tau}.
          \end{equation*}
    \item \textbf{Line Integral over Scalar Field \(f\)}
          \begin{equation*}
              \int_{\mathscr{C}} f \odif{s} = \int_{\mathscr{C}} f \odv{s}{t} \odif{t} = \int_a^b f\left( \symbf{r}\left( t \right) \right) \norm*{\symbf{r}'\left( t \right)} \odif{t}.
          \end{equation*}
    \item \textbf{Line Integral over Vector Field \(\symbf{F}\)}
          \begin{equation*}
              \int_{\mathscr{C}} \symbf{F} \cdot \odif{\symbf{s}} = \int_{\mathscr{C}} \symbf{F} \cdot \odv{\symbf{s}}{t} \odif{t} = \int_a^b \symbf{F} \cdot \symbf{r}'\left( t \right) \odif{t}.
          \end{equation*}
\end{itemize}
For a surface \(\mathscr{S}\) defined by the parametric function
\(\symbf{r}\left( s,\: t  \right) = \begin{bmatrix}
    x\left( s,\: t \right) \\
    y\left( s,\: t \right) \\
    z\left( s,\: t \right)
\end{bmatrix}\)
on the intervals \(a \leqslant s \leqslant b\) and \(c \leqslant t \leqslant d\):
\begin{itemize}
    \item \textbf{Surface Area}
          \begin{equation*}
              A = \iint_{\mathscr{S}} \odif{\sigma} = \int_c^d \int_a^b \norm*{\symbf{r}_s \times \symbf{r}_t} \odif{s} \odif{t}.
          \end{equation*}
    \item \textbf{Surface Integral over Scalar Field \(f\)}
            \begin{equation*}
                \iint_{\mathscr{S}} f \odif{\sigma} = \int_c^d \int_a^b f \norm*{\symbf{r}_s \times \symbf{r}_t} \odif{s} \odif{t}.
            \end{equation*}
    \item \textbf{Surface Integral over Vector Field \(\symbf{F}\)}
            \begin{equation*}
                \iint_{\mathscr{S}} \symbf{F} \cdot \odif{\symbf{\sigma}} = \int_c^d \int_a^b \symbf{F} \cdot \left( \symbf{r}_s \times \symbf{r}_t \right) \odif{s} \odif{t}.
            \end{equation*}
\end{itemize}
where \(\symbf{n} = \symbf{r}_s \times \symbf{r}_t\) is a normal
vector to the surface.
\end{document}
